{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a deep learning model from scractch for non linear classification problem\n",
    " \n",
    "There are four steps to build and use a machine learning model.\n",
    "\n",
    "- **Preprocessing** : feature selection, ramdomize, normalize, shuffle, EDA, split train test\n",
    "- **Learning** : implementing the model\n",
    "- **Evaluation** : metrics ( true positive, false positive...etc), tests, overfitting undertitting, hyper tunnin\n",
    "- **Prediction** on new unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem context\n",
    "\n",
    "You will implement a deep learning model to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\n",
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 -  Preprocessing et EDA\n",
    "\n",
    "La première étape consiste à charger puis visualizer puis analyser les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#fonction utilitaire pour afficher les points\n",
    "\n",
    "def plot_points(X, y):\n",
    "    sns.scatterplot(X[:, 0], X[:, 1], hue=y.flatten(), cmap=plt.cm.coolwarm, s=30, edgecolors='k')\n",
    "    plt.show()\n",
    "    \n",
    "# fonction utilitaire pour randomiser les données (shuffle)\n",
    "def randomize(X, Y):\n",
    "    permutation = np.random.permutation(Y.shape[0])\n",
    "    X2 = X[permutation,:]\n",
    "    Y2 = Y[permutation]\n",
    "    Y2[Y2 == -1] = 0 \n",
    "    return X2, Y2  \n",
    "\n",
    "# fonction utilitaire pour split les données en test et train sets\n",
    "def split_train_test(data, test_size):\n",
    "    sample = np.random.choice(data.index, size=int(len(data)*(1 - test_size)), replace=False)\n",
    "    train_data, test_data = data.iloc[sample], data.drop(sample)\n",
    "    return train_data, test_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/hzitoun/workshop-deep-learning-tensorflow/master/microships_QA.csv');\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test sets\n",
    "\n",
    "In order to test our algorithm, we'll split the data into a Training and a Testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_train_test(data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_data.drop('result', axis=1))\n",
    "y_train = np.array(train_data['result']).reshape(-1, 1)\n",
    "X_test = np.array(test_data.drop('result', axis=1))\n",
    "y_test = np.array(test_data['result']).reshape(-1, 1)\n",
    "\n",
    "print(\"Shape of X_train = \", X_train.shape, \"Shape of y_train=\", y_train.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize features and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = randomize(X_train, y_train)\n",
    "X_test, y_test = randomize(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 -  Implementing the deep learning model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sigmoid\n",
    "**In order to learn using gradient descent** error function has to be continuous and differentiable.\n",
    "- Sigmoid function gives not discrete value but probability of being a value (probabilty space). Sigmoid activation function is used when we have 2 labels. \n",
    "\n",
    "$$sigmoid(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    # TODO \n",
    "\n",
    "def sigmoid_prime(X):\n",
    "    # TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "We will use MSA = MEAN SQUARED ERROR\n",
    "\n",
    "It is easier to actually optimize on MSE, as the a quadratic term is differentiable. This factor makes this metric better for gradient based optimization algorithms.\n",
    "\n",
    "$$E = \\frac{1}{2 N} \\sum (y_p -y_t)^2 $$\n",
    "\n",
    "Derivative of Mean Squared Error \n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_j} = -\\frac{1}{N}\\sum_{n=1}^N x_{nj}(y_n - (Xw)_n) = -\\frac{1}{N}x_j^T (y - Xw),\n",
    "$$\n",
    "\n",
    "Weight error \n",
    "\n",
    "$$\n",
    "\\nabla E(w) = -\\frac{1}{N}X^T (y - Xw)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    # TODO\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer\n",
    "A layer in neural networks contains a set neurons and has two methods: forward_prog and backward_prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let us implement a fully connected layer**\n",
    "\n",
    "- Output (prediction) formula\n",
    "\n",
    "$$\\hat{y} = activate(input * w + b) =  \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n",
    "\n",
    "- Backpropagation with gradient descent\n",
    "\n",
    "$$ w_i \\longrightarrow w_i - \\alpha \\frac{dE}{dw_i}$$\n",
    "\n",
    "$$ b \\longrightarrow b - \\alpha \\frac{dE}{db}$$\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://user.oc-static.com/upload/2017/10/21/15085704733297_P2C1-4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "      # TODO Init weights and bias\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        # TODO implement forward propagation (input * weigths + bias)\n",
    "       \n",
    "    \n",
    "    \n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        #TODO caluclate input_error, weights_error, and bias_error\n",
    "        \n",
    "\n",
    "        # update parameters\n",
    "        # TODO update params\n",
    "       \n",
    "        \n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        \n",
    "        # TODO output activation of input\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have all the elements to implement a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, loss, loss_prime):\n",
    "        self.layers = [] \n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "        self.errors = []\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # train the network using the gradient descent algorithm\n",
    "    def train(self, x_train, y_train, epochs, learning_rate):\n",
    "        \n",
    "        # transform to tensor (3 dimensions matrix)\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "        y_train = y_train.reshape(y_train.shape[0], 1, y_train.shape[1])\n",
    "        \n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "        # training loop\n",
    "        for epoch in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                \n",
    "                \n",
    "                # TODO forward propagation\n",
    "                \n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                # TODO backward propagation\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            \n",
    "            self.errors.append(err)\n",
    "            \n",
    "            if (epoch % 100 == 0):\n",
    "                print('epoch %d/%d   error=%f' % (epoch, epochs, err))\n",
    "            \n",
    "    # predict output for given input\n",
    "    def predict(self, input_data, threshold = 0.5):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "        \n",
    "            # TODO append to result the ouput of each layer's backpropagation\n",
    "\n",
    "        return (np.asarray(result) > threshold).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_schematic.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement a single layer one neuron neural network\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the error\n",
    "plt.title(\"Error Plot\")\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.plot(net.errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 -  Evaluate \n",
    "\n",
    "Once we've trained our neural network, we need to evaluate our model on test data: accuracy, false positive, true positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction utilitaire pour afficher l'accuracy de notre model deep learning\n",
    "def print_accuracy() : \n",
    "    predictions = net.predict(X_test, threshold = 0.5)\n",
    "    accuracy = np.multiply(np.mean((predictions == y_test.flatten()).astype(int)), 100)\n",
    "    print(\"Accuracy: {:.3f}%\".format(accuracy))\n",
    "    \n",
    "# fonction utilitaire pour generer un grid 2-D de coordonnées \n",
    "# c'est une façon d'échantillonner une fonction sur une grille rectangulaire, \n",
    "# ça aide à visualiser la fonction comme une \"image\".\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "# cette fonction permet de visualiser la limite de descision de notre model\n",
    "def plot_descision_boundary(estimator, X, y):\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "    target = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = estimator.predict(target)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "    sns.scatterplot(X0, X1, hue=y.flatten(), cmap=plt.cm.coolwarm, s=30, edgecolors='k')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the descision boundary to get a visualization of what our neural networks have learned.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_descision_boundary(net, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 -  Predict on new unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.predict(np.array([[-2, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate qu'un réseau de neurones avec un seul neuronne ne peut apprendre qu'une fonction lineaire W * X +b\n",
    "Donc résoudre notre problème de classification il faut que notre reseau de neurones apprenne une fonction non lineaire et pour faire cela on va combiner le resultat de plusieurs neuronnes ensemble.\n",
    "\n",
    "\n",
    "<img src=\"imgs/neural_network_non_linear.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement two layers neural network\n",
    "\n",
    "# Plotting the error\n",
    "plt.title(\"Error Plot\")\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.plot(net.errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_descision_boundary(net, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On commence à apprendre des fonctions non linéaire mais on est encore loin du résultat attendu. On a joute donc encore plus de neurones.**\n",
    "\n",
    "<img src=\"imgs/neural_network_multi_dim.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO Implement two layers with multi neuron neural network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the error\n",
    "plt.title(\"Error Plot\")\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.plot(net.errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_descision_boundary(net, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
