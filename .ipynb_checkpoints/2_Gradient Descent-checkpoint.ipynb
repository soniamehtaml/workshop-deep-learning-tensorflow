{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation de l'agorithme de Gradient Descent\n",
    "\n",
    "Maitenant, on va implémenter les fonctions de base de l'agorithme de gradient descent pour trouver la barrière de descision. On commence avec des fonctions qui vont vous aider à visualiser les donnéés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def plot_points(X, y):\n",
    "    good_choice = X[np.argwhere(y==1).flatten()]\n",
    "    bad_choice = X[np.argwhere(y==0).flatten()]\n",
    "    plt.scatter(bad_choice[:, 0], bad_choice[:, 1], s = 50, color = 'red', marker='x')\n",
    "    plt.scatter(good_choice[:, 0], good_choice[:, 1], s = 50, color = 'green', marker=\"^\")\n",
    "\n",
    "def display(m, b, color='g--'):\n",
    "    plt.xlim(-0.05,1.05)\n",
    "    plt.ylim(-0.05,1.05)\n",
    "    x = np.arange(-10, 10, 0.1)\n",
    "    plt.plot(x, m*x+b, color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAFipJREFUeJzt3X+MHPd53/H3QylSYic2pYhyKckSKUVJLbRwrbtCUtyWqWI3tmDYx1MCqDYaNlIhoC2sI9sykWCgTYAWiNMiFAUUdoU6rJrIv6LcnQihqcsqRvuX5R6t2JZyVMXUFM2Ito6oRZdlAVnep3/MrLk83vGOt7c7s997v4DD7nxn7va5ud3Pzj4zNxOZiSSpXJuaLkCSNFgGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwlzddAMA111yT27Zta7oMSRophw8fPpWZW1ZarhVBv23bNubm5pouQ5JGSkS8sprlbN1IUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16ShqSTHQ48f4BOdob6uAa9JA3J9Pw09x+8n5n5maE+rkEvSUPQyQ57D+0FYO+hvUPdqjfoJWkIpuenOXX2FAALZxeGulVv0Etak6b6zaOouzV/5o0zAJx548xQt+pXDPqI+L2IeC0iXugZuzoiDkXEy/XtVfV4RMRjEXE0Ir4REbcPsnhJzWmq3zyKerfmu4a5Vb+aLfr/AHxg0djDwLOZeSvwbD0N8EHg1vrrQeBT61OmpDZpst88ahZvzXcNc6t+xaDPzP8O/O9Fwx8BnqjvPwFM9Iz/x6x8BdgcEVvXq9h++DFTWj9N9ptHzeyRWY6fPr7kvOOnjzN7ZHbgNaz1fPTvyMyTAJl5MiKurcevB77ds9yJeuzk4h8QEQ9SbfVz4403rrGM1et+zHzblW/j3tvuHfjjSaVart+881072RTu9lvsprffxO47dl90/qCt94VHYomxXGrBzHwceBxgfHx8yWXWy+KPmT4hpbW7WL/ZjagLjV03xth1Y43WsNa0+263JVPfvlaPnwDe2bPcDcCray9vffgxU1ofbeg369KtNegPArvq+7uAp3vGf7U++uZO4HS3xdOUpg9rWlyL+wk0ytrQb9alW7F1ExGfA34BuCYiTgD/Avht4IsR8QBwHPiVevH/BNwDHAXOAr82gJovSZs+ZrqfQKOuDf1mXbrIHGh7fFXGx8dzEBcH72SHWx67hWOvH7tg3vbN2zn60NGh9ep7axn2Y0sqU0QczszxlZYrOmna9DHT/QSSmrLeR920Sls+Zno4mqQmFR30bTisCdq1n0DSxuPm5IB5OJqkphn0A9am/QSSNqaiWzdt0Jb9BJI2LoN+wNqyn0DSxmXrRpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMeqkhnexw4PkDdLLTdCkqnEEvNWR6fpr7D97PzPxM06WocAa91IBOdth7aC8Aew/tHZ2t+kyYmaluVzNeihH/vfsK+ojYExEvRsQLEfG5iPjxiNgeEc9FxMsR8YWIuGK9ipVKMT0/zamzpwBYOLswOlv1s7MwOQl79pwLt8xqenKyml+iEf+91xz0EXE98BAwnpl/BbgMuA/4JLAvM28Fvgc8sB6FSqXobs2feeMMAGfeODM6W/UTEzA1Bfv3nwu9PXuq6ampan6JRvz3vnwdvv8nIuIHwFuAk8DdwEfr+U8Avwl8qs/HkYrRuzXf1d2qv/e2exuqapUiYN++6v7+/dUXVGG3b181v0Qj/ntH9tFbiogp4F8B/w/4L8AU8JXM/Jl6/juBP663+Bd/74PAgwA33njj2CuvvLLmOqRR0ckOtzx2C8deP3bBvO2bt3P0oaNsihHYdZYJm3rq7HRaH3bromW/d0QczszxlZbrp3VzFfARYDtwHfBW4INLLLrkO0lmPp6Z45k5vmXLlrWWIY2U2SOzHD99fMl5x08fZ/ZIu3u9wLm2Ra/e3nWpRvj37qd18z7gW5m5ABAR08DPA5sj4vLMfBO4AXi1/zKlMtz09pvYfcfui85vtcW96X37zk3DSLQx1mTUf+/MXNMXcAfwIlVvPqj68R8H/hC4r17m08A/WulnjY2NpaQenU7m9HR1u5rxYZmezoTMqalzNXQ61TRU80vU0t8bmMvV5PVqFlr2m+G3gCPAC8DvA1cCNwNfBY7WoX/lSj/HoJcWaWmwtPYNaNBa+nuvNuj72hm7XsbHx3Nubq7pMqT2uFirYESO9NDgrXZnbL+HV0oahBE/nE/t4ha91GYtO5xP7TLwwyslDdgIH86ndjHopTZa3KPvdC78F3xplezRS200O3vhjtfenv2OHbBzZ7M1amQY9FIbTUzA9HR12+3Jd8N+x47Wn0RL7WLQS20UsfQW+3Lj0kXYo5ekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+g3uE52OPD8gdG4MLWkNTHoN7jp+WnuP3g/M/MzTZciaUAM+g2skx32HtoLwN5De92qlwpl0G9g0/PTnDp7CoCFswtu1UuFMugHYBT63t2t+TNvnAHgzBtn3KqXCmXQD8Ao9L17t+a73KqXymTQr7NR6Hsv3prvcqteKpNBv85Goe89e2SW46ePLznv+OnjzB6ZHXJFkgbJ0xSvo+X63jvftZNN0Z731JvefhO779h90fmSymHQr6OL9b3vve3ehqq60Nh1Y4xdN9Z0GZKGpD2bmSPOvrektjLo14l9b0ltZetmndj3ltRWBv06se8tqa1s3UhS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIK11fQR8TmiHgqIo5ExHxE3BURV0fEoYh4ub69ar2KXckoXNlJ6ywTZmaq29WMS8PWgudov1v0+4H/nJl/GXg3MA88DDybmbcCz9bTQzEKV3bSOpudhclJ2LPn3Asms5qenKzmS01qw3M0M9f0BbwN+BYQi8ZfArbW97cCL630s8bGxrJfP+z8MLc9ui35TXL7o9vzh50f9v0zNQI6ncypqUyobpealpo0wOcoMJeryOt+znVzM7AAHIiIdwOHgSngHZl5sn4TORkR1/bxGKu21JWd2nQOeA1IBOzbV93fv7/6ApiaqsYjmqtNglY8RyPX2B+KiHHgK8B7M/O5iNgPfB/4eGZu7lnue5l5QZ8+Ih4EHgS48cYbx1555ZU11QFVb/6Wx27h2OvHfjS2ffN2jj50tFVXdtIAZcKmnr91p2PIq10G8ByNiMOZOb7Scv2k4AngRGY+V08/BdwOfDcittZFbAVeW+qbM/PxzBzPzPEtW7b0UcbFr+ykDaDb7+zV2w+Vmtbwc3TNQZ+Z3wG+HRE/Vw/9IvBnwEFgVz22C3i6rwpX4JWdNrjuC2j//uqjcKdT3e7fb9irHVrwHO33fPQfB56MiCuA/wX8GtWbxxcj4gHgOPArfT7GRa3myk6T75ocZAlq0uzsuRdQt9/Z2w/dsQN27my2Rm1sLXiOrrlHv57Gx8dzbm5uTd97+NXDfPabn112/kf/6ke9IEjJMqsX0sTE+f3O5calYRvgc3S1PfqRD3pJ2qiGsTNWkjQCDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0A9KC64qI0lg0A9OG64qI0n0f1IzLWdi4twZ6qA6iVHvGewmJpqtT9KGYdAPSguuKiNJ4EnNBs8rH0kaEE9q1gZe+UhSCxj0g9KCq8pIEtijH5wWXFVGksCgH5yJCZiePv/qMd2w37HDo24kDY1BPygRS2+xLzcuSQNij16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhdsYQZ8JMzMXngN+uXFJKsjGCPrZWZicPP+CH90Lg0xOVvMlqVAb4zTFExPnru4E1Tnhe6/+5LnhJRVsYwT94qs7dQO/9+pPklSoyBb0p8fHx3Nubm7wD5QJm3q6VZ2OIS9pZEXE4cwcX2m5vnv0EXFZRDwfEc/U09sj4rmIeDkivhARV/T7GOui25Pv1YaLdLujWNKArcfO2Clgvmf6k8C+zLwV+B7wwDo8Rn+6Id/tyXc653r2TYe9O4olDVpmrvkLuAF4FrgbeAYI4BRweT3/LuBLK/2csbGxHKjp6UzInJrK7HSqsU6nmoZqflN66+jWt3hakpYAzOUqsrrfnbGPAr8O/FQ9/dPA65n5Zj19Ari+z8fo38QETE9Xt92efHcH7Y4dzR51445iSQO25tZNRHwIeC0zD/cOL7Hokn2RiHgwIuYiYm5hYWGtZaxOBOzceWFoLjc+bL1h32XIS1on/fTo3wt8OCKOAZ+nat88CmyOiO4nhRuAV5f65sx8PDPHM3N8y5YtfZRRgLbuKJZUhDUHfWY+kpk3ZOY24D7gTzLzY8CXgV+uF9sFPN13lSVr845iSUUYxD9M/Qbw+Yj4l8DzwGcG8BjlmJ09F/Lddk1vz37Hjqq9JElrtLH+YaqNMquw791RfLFxSaqt9h+mNsYpENqsu0N4teOSdIk2xtkrJWkDM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHqNlkyYmaluVzMuyaDXiJmdhclJ2LPnXKhnVtOTk9V8See5vOkCpEsyMQFTU7B/fzW9b18V8vv3V+MTE83WJ7WQQa/RElGFO1Th3g38qalqPKK52qSWimxBT3N8fDzn5uaaLkOjJBM29XQeOx1DXhtORBzOzPGVlrNHr9HT7cn36u3ZSzqPQa/R0g35bk++0znXszfspSXZo9domZ09F/Ldnnxvz37HDti5s9kapZYx6DVaJiZgerq67fbku2G/Y4dH3UhLMOg1WiKW3mJfblySPXpJKp1BL0mFW3PQR8Q7I+LLETEfES9GxFQ9fnVEHIqIl+vbq9avXEnSpepni/5N4J9m5ruAO4F/HBG3AQ8Dz2bmrcCz9bQkqSFrDvrMPJmZX6vv/x9gHrge+AjwRL3YE4CHQUhSg9alRx8R24D3AM8B78jMk1C9GQDXrsdjSJLWpu+gj4ifBP4I2J2Z37+E73swIuYiYm5hYaHfMorSyQ4Hnj9AJztNlyKpAH0FfUT8GFXIP5mZ0/XwdyNiaz1/K/DaUt+bmY9n5nhmjm/ZsqWfMoozPT/N/QfvZ2Z+pulSJBWgn6NuAvgMMJ+Zv9sz6yCwq76/C3h67eVtPJ3ssPfQXgD2HtrrVr2kvvWzRf9e4O8Bd0fEn9Zf9wC/Dbw/Il4G3l9Pa5Wm56c5dfYUAAtnF9yql9Q3z0ffIp3scMtjt3Ds9WM/Gtu+eTtHHzrKpvB/2ySdz/PRj6Derfkut+ol9cugb4lub/7MG2fOGz/zxhl79ZL6MvpBnwkzMxdecGK58ZaaPTLL8dPHl5x3/PRxZo/MDrkiaQMrJFe6Rv80xbOzMDl5/oUoeq9CND09EqevventN7H7jt0XnS9pSArJlR/JzMa/xsbGcs06ncypqUyobpealqRLMSK5AszlKjK2jKNuet9pu3rfiSXpUo1Arqz2qJsygh6qP8qmnl0OnU5r/hiSRlTLc2VjHV7ZfefttWfPyO0wkdQiBeXK6Ad978erqanqHXdqqpoe0T+KpIYVlitlHHXT/WN0e2f79lXz9u+HHTtGa++4pOYVliuj36PPrP4oExPn986WG5eklYxIrmy8nbGStMFsrJ2xkqRlGfSSVDiDXpIKZ9BLUuFasTM2IhaAV1ZY7Brg1ArLNKWttVnXpWtrbdZ16dpa23rWdVNmrnjR7VYE/WpExNxq9i43oa21Wdela2tt1nXp2lpbE3XZupGkwhn0klS4UQr6x5su4CLaWpt1Xbq21mZdl66ttQ29rpHp0UuS1maUtuglSWswEkEfER+IiJci4mhEPNxgHe+MiC9HxHxEvBgRU/X41RFxKCJerm+vaqi+yyLi+Yh4pp7eHhHP1XV9ISKuaKiuzRHxVEQcqdfdXW1YZxGxp/47vhARn4uIH29qnUXE70XEaxHxQs/YkusoKo/Vr4dvRMTtQ67rX9d/y29ExExEbO6Z90hd10sR8UuDqmu52nrm/bOIyIi4pp5udJ3V4x+v18uLEfE7PeODX2erud5gk1/AZcCfAzcDVwBfB25rqJatwO31/Z8C/idwG/A7wMP1+MPAJxuq758AnwWeqae/CNxX3/808A8bqusJ4B/U968ANje9zoDrgW8BP9Gzrv5+U+sM+FvA7cALPWNLriPgHuCPgQDuBJ4bcl1/B7i8vv/Jnrpuq1+fVwLb69ftZcOsrR5/J/Alqv/NuaYl6+xvA/8VuLKevnaY62zgT+B1WGl3AV/qmX4EeKTpuupangbeD7wEbK3HtgIvNVDLDcCzwN3AM/UT+lTPC/K89TjEut5WB2osGm90ndVB/23gaqrrMjwD/FKT6wzYtigcllxHwL8D/u5Syw2jrkXzdgJP1vfPe23WYXvXMNdZPfYU8G7gWE/QN7rOqDYg3rfEckNZZ6PQuum+ILtO1GONiohtwHuA54B3ZOZJgPr22gZKehT4daBTT/808HpmvllPN7XebgYWgAN1W+nfR8RbaXidZeZfAP8GOA6cBE4Dh2nHOutabh216TVxP9WWMrSgroj4MPAXmfn1RbOaru1ngb9ZtwX/W0T89WHWNQpBv9TZ/Rs9VCgifhL4I2B3Zn6/yVrqej4EvJaZh3uHl1i0ifV2OdXH2E9l5nuA/0vVhmhU3e/+CNXH5euAtwIfXGLRNh6W1oq/bUR8AngTeLI7tMRiQ6srIt4CfAL450vNXmJsmOvscuAqqrbRXuCLERHDqmsUgv4EVc+t6wbg1YZqISJ+jCrkn8zM6Xr4uxGxtZ6/FXhtyGW9F/hwRBwDPk/VvnkU2BwR3ctFNrXeTgAnMvO5evopquBvep29D/hWZi5k5g+AaeDnacc661puHTX+moiIXcCHgI9l3XNoQV23UL1xf71+LdwAfC0i/lILajsBTGflq1SfvK8ZVl2jEPT/A7i1PhriCuA+4GAThdTvwJ8B5jPzd3tmHQR21fd3UfXuhyYzH8nMGzJzG9X6+ZPM/BjwZeCXm6qrru07wLcj4ufqoV8E/oyG1xlVy+bOiHhL/Xft1tX4Ouux3Do6CPxqfSTJncDpbotnGCLiA8BvAB/OzLOL6r0vIq6MiO3ArcBXh1VXZn4zM6/NzG31a+EE1cET36HhdQbMUm2AERE/S3VQwimGtc4GuaNkHXds3EN1hMufA59osI6/QfWx6hvAn9Zf91D1w58FXq5vr26wxl/g3FE3N9dPmqPAH1Lv8W+gpr8GzNXrbZbqI2zj6wz4LeAI8ALw+1RHPjSyzoDPUe0r+AFVQD2w3Dqi+rj/b+vXwzeB8SHXdZSqr9x9DXy6Z/lP1HW9BHxw2Ots0fxjnNsZ2/Q6uwL4g/q59jXg7mGuM/8zVpIKNwqtG0lSHwx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIK9/8BRPyBF3RImu4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('viager.csv')\n",
    "X = np.array(data.drop('y', axis=1))\n",
    "y = np.array(data['y'])\n",
    "plot_points(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implementing the basic functions\n",
    "Here is your turn to shine. Implement the following formulas, as explained in the text.\n",
    "- Sigmoid activation function\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- Output (prediction) formula\n",
    "\n",
    "$$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n",
    "\n",
    "- Error function\n",
    "\n",
    "$$Error(y, \\hat{y}) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$$\n",
    "\n",
    "- The function that updates the weights\n",
    "\n",
    "$$ w_i \\longrightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n",
    "\n",
    "$$ b \\longrightarrow b + \\alpha (y - \\hat{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the following functions\n",
    "\n",
    "# Activation (sigmoid) function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def output_formula(features, weights, bias):\n",
    "    return sigmoid(np.dot(features, weights) + bias)\n",
    "\n",
    "def error_formula(y, output):\n",
    "    return - y*np.log(output) - (1 - y) * np.log(1-output)\n",
    "\n",
    "def update_weights(x, y, weights, bias, learnrate):\n",
    "    output = output_formula(x, weights, bias)\n",
    "    d_error = -(y - output)\n",
    "    weights -= learnrate * np.dot(x.T, d_error)\n",
    "    bias -= learnrate * d_error\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n",
    "This function will help us iterate the gradient descent algorithm through all the data, for a number of epochs. It will also plot the data, and some of the boundary lines obtained as we run the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(44)\n",
    "\n",
    "epochs = 100\n",
    "learnrate = 0.01\n",
    "\n",
    "def train(features, targets, epochs, learnrate, graph_lines=False):\n",
    "    \n",
    "    errors = []\n",
    "    n_records, n_features = features.shape\n",
    "    last_loss = None\n",
    "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "    bias = 0\n",
    "    for e in range(epochs):\n",
    "        del_w = np.zeros(weights.shape)\n",
    "        for x, y in zip(features, targets):\n",
    "            output = output_formula(x, weights, bias)\n",
    "            error = error_formula(y, output)\n",
    "            weights, bias = update_weights(x, y, weights, bias, learnrate)\n",
    "        \n",
    "        # Printing out the log-loss error on the training set\n",
    "        out = output_formula(features, weights, bias)\n",
    "        loss = np.mean(error_formula(targets, out))\n",
    "        errors.append(loss)\n",
    "        if e % (epochs / 10) == 0:\n",
    "            print(\"\\n========== Epoch\", e,\"==========\")\n",
    "            if last_loss and last_loss < loss:\n",
    "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "            else:\n",
    "                print(\"Train loss: \", loss)\n",
    "            last_loss = loss\n",
    "            predictions = out > 0.5\n",
    "            accuracy = np.mean(predictions == targets)\n",
    "            print(\"Accuracy: \", accuracy)\n",
    "        if graph_lines and e % (epochs / 100) == 0:\n",
    "            display(-weights[0]/weights[1], -bias/weights[1])\n",
    "            \n",
    "\n",
    "    # Plotting the solution boundary\n",
    "    plt.title(\"Solution boundary\")\n",
    "    display(-weights[0]/weights[1], -bias/weights[1], 'black')\n",
    "\n",
    "    # Plotting the data\n",
    "    plot_points(features, targets)\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the error\n",
    "    plt.title(\"Error Plot\")\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.ylabel('Error')\n",
    "    plt.plot(errors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train the algorithm!\n",
    "When we run the function, we'll obtain the following:\n",
    "- 10 updates with the current training loss and accuracy\n",
    "- A plot of the data and some of the boundary lines obtained. The final one is in black. Notice how the lines get closer and closer to the best fit, as we go through more epochs.\n",
    "- A plot of the error function. Notice how it decreases as we go through more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (1,) not aligned: 2 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-90005167daac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearnrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-b30d617fa49c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(features, targets, epochs, learnrate, graph_lines)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_formula\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_formula\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearnrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Printing out the log-loss error on the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-200d998ef55b>\u001b[0m in \u001b[0;36mupdate_weights\u001b[0;34m(x, y, weights, bias, learnrate)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_formula\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0md_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearnrate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mbias\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearnrate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,) and (1,) not aligned: 2 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "train(X, y, epochs, learnrate, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return 1 / (1 + np.exp(-x));\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return tanh(x) * (1 - tanh(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            # print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.55\n"
     ]
    }
   ],
   "source": [
    "X = X.reshape(20, 1, 2)\n",
    "y = y.reshape(20, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "# network\n",
    "net = Network()\n",
    "net.add(FCLayer(2, 2))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(2, 2))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(2, 1))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(X, y, epochs=500, learning_rate=0.1)\n",
    "\n",
    "# predict\n",
    "out = np.asarray(net.predict(X))\n",
    "\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34932500, 2)\n"
     ]
    }
   ],
   "source": [
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "features = np.c_[xx.ravel(), yy.ravel()]\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "features.reshape(features.shape[0], 1, 2)\n",
    "Z = np.asarray(net.predict(features))\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.4)\n",
    "sns.scatterplot(X0, X1, hue=y, cmap=plt.cm.coolwarm, s=100, edgecolors='k')\n",
    "# plt.xlim(0, 1)\n",
    "# plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
